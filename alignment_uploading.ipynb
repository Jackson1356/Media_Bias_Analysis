{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91fba7ba-d3cb-4812-835c-434edf98941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 16:18:50.211784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-03 16:18:50.521873: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-03 16:18:50.524708: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-03 16:18:50.909879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-03 16:18:52.975837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.cluster import DBSCAN\n",
    "import pickle\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "from retinaface import RetinaFace\n",
    "from imutils import build_montages\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e62d99-c83d-4718-897a-6ec6cf73740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"./sylvan-airship-406701-7556ece80fd0.json\")\n",
    "client = storage.Client(credentials=credentials)\n",
    "bucket = client.get_bucket('full_images_2024')\n",
    "blobs = bucket.list_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a79fce1-27c1-47e9-aa5e-ac6edfe9349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_from_directory(outlets, start_index=0, max_images=2000):\n",
    "    for outlet in outlets:\n",
    "        pre = outlet + '/' + outlet + '2018'\n",
    "        count = 0\n",
    "        blobs = bucket.list_blobs(prefix=pre)\n",
    "        for blob in blobs:            \n",
    "            if count < start_index:\n",
    "                count+=1\n",
    "                continue\n",
    "            if count >= max_images:\n",
    "                break\n",
    "        \n",
    "            print(f\"{outlet}: {count}, name: {blob.name}\")\n",
    "            \n",
    "            with BytesIO(blob.download_as_bytes()) as image_stream:\n",
    "                with Image.open(image_stream) as image:\n",
    "                    image = image.convert('RGB')  # Convert to RGB if not already\n",
    "        \n",
    "                    img_name = blob.name.split(\"/\")[-1]\n",
    "                    img_format = img_name.split(\".\")[-1]\n",
    "        \n",
    "                    np_image = np.array(image)  # Only convert to numpy array when necessary\n",
    "\n",
    "                    face_objs = DeepFace.extract_faces(\n",
    "                        img_path=np_image, \n",
    "                        detector_backend=\"yolov8\",\n",
    "                        enforce_detection = False,\n",
    "                        \n",
    "                    )\n",
    "                    print(f\"number of faces: {len(face_objs)}\")\n",
    "                    n = 1\n",
    "                    for face in face_objs:\n",
    "                        if face['confidence'] < 0.3:\n",
    "                            continue\n",
    "                        x, y, w, h = face['facial_area']['x'], face['facial_area']['y'], face['facial_area']['w'], face['facial_area']['h']\n",
    "                        cropped_image = image.crop((x, y, x + w, y + h))\n",
    "                        np_cropped_image = np.array(cropped_image)\n",
    "                        cropped_img_name = f'{img_name.split(\".\")[0]}_alignment_{n}.{img_format}'\n",
    "                        outlet_year = img_name.split(\"_\")[0]\n",
    "                        n += 1\n",
    "\n",
    "                        for model in models:\n",
    "                            embeddings[model][cropped_img_name] = []\n",
    "                            embedding = DeepFace.represent(\n",
    "                                img_path = np_cropped_image,\n",
    "                                model_name = model,\n",
    "                                detector_backend = 'yolov8', \n",
    "                                align = False,\n",
    "                                enforce_detection = False,\n",
    "                            )\n",
    "                            for embed in embedding:\n",
    "                                embeddings[model][cropped_img_name].append(embed['embedding'])\n",
    "        \n",
    "                        # Save to a BytesIO stream to avoid using local storage\n",
    "                        cropped_stream = BytesIO()\n",
    "                        cropped_image.save(cropped_stream, format=img_format)\n",
    "                        cropped_stream.seek(0)\n",
    "        \n",
    "                        # Upload directly from BytesIO stream\n",
    "                        new_blob = bucket.blob(f\"alignment/{outlet}/{outlet_year}/{cropped_img_name}\")\n",
    "                        new_blob.upload_from_file(cropped_stream)\n",
    "        \n",
    "                        # Clear stream memory\n",
    "                        cropped_stream.close()\n",
    "\n",
    "        \n",
    "                    if len(face_objs) == 0:\n",
    "                        del np_image, face_objs, image, np_cropped_image, cropped_image\n",
    "                    else:\n",
    "                        del np_image, face_objs, image\n",
    "                    gc.collect()\n",
    "                    count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea9df9-db43-4bdd-9d95-34173b6865bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "  \"VGG-Face\", \n",
    "  \"Facenet\", \n",
    "  \"Facenet512\", \n",
    "]\n",
    "embeddings = {model: {} for model in models}\n",
    "# embeddings = pickle.loads(open(\"embeddings_alignment\", \"rb\").read())\n",
    "outlets = [\"gatewaypundit\", \"huffpost\", \"nyt\"]\n",
    "process_images_from_directory(outlets, start_index = 0, max_images=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4801faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_alignment_5\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947dd5be-9abb-4156-b040-6d3a21151667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings['VGG-Face'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
